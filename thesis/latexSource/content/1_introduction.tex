\chapter{Introduction}

The complexity of software architecture is growing with rapid speed and we cannot expect the incidence of failures or potential problems to decrease at the same rate. Therefore, modern systems are relying heavily on anomaly detection principles which can help with problem detection and in the best case even with their prediction for the potential design of self-healing systems such as the one described in \cite{dashofy2002towards}.

Anomalies, often referred to, in other publications as outliers, exceptions or aberrations, may continually occur everywhere around us, whether we are able to distinguish them from standard behaviour depends only on our perspective and knowledge. It means, an event by itself is never an anomaly, to classify it we always need to know the context in which it occurs and , preferably, even some relevant history. To be more general, Cambridge dictionary defines an anomaly as follows\footnote{\url{http://dictionary.cambridge.org/dictionary/english/anomaly}}:

\begin{center}\textit{A person or thing that is different from what is usual, or not in agreement with something else and therefore not satisfactory. }
\end{center}

But a more suitable for definition for us could be this one from an anomaly detection survey of 2009~\cite{chandola2009anomaly}:

\begin{center}
\textit{Anomalies are patterns in data that do not conform to a well-defined notion of normal behavior.}
\end{center}

It means, an anomaly may be a number of a time series which does not belong there or for which the probability of observing is too low, according to expected rules or statistics. It can be a state of a state space which has previously been classified as an anomaly, has never been seen before or does not make a sense in current execution. It can be a sequence of actions which are not supposed to follow each other and there exist many other examples of patterns which do not fit to its context.

A nice general example of an anomaly would be an e-shop where every transaction is logged and a time line of transaction per day is drawn. Such line will have, most probably, a week pattern with an increasing trend and slight fluctuation. You would be able to predict an amount of transaction with a certain probability level for most of days in year. But for instance, during a Black sale Friday the amount of transactions will go up. If a third party witnessed such a time line without context, they could expected that something went wrong (in a positive way), but an experienced business manager would know, it is normal and people are simply buying more goods.

Monitoring and problematic of anomaly detection in different forms is studied across multiple fields and domains. In statistics, this problem has been studied for long time with its roots in the 19\textsuperscript{th} century - a historical paper \cite{edgeworth1887xli} from 1887. There are many studies and implementations which deal with some form of anomaly or outlier detection to date. Let us highlight some to demonstrate possible implementations: \cite{salem2014online} introduces a health-care-focused alerting system for wear\ms{-}able devices, \cite{rehak2008trust} detects network traffic anomalies to discovers security threats, \cite{bolton2001unsupervised} studies credit card fraud detection and there are many other papers. The one thing all implementations have in common is that every field demands a specific approach and specific background knowledge. In other words, there is no universal algorithm or a solution which would be applicable to all problems. In general, a deep knowledge of data and its comprehension is the main and biggest key to finding the best anomaly detection approach.

This thesis focuses on complex software architectures and runtime anomalous behaviour. Most of current cloud IT solutions and architectures are, like walls are built from bricks, built from multiple existing sub-components or applications - nobody develops from scratch, since it is easier and cheaper to use well-working sub-solutions to achieve new additional value. This means that, mandatory attributes such as stability and quality of new solutions do not only depend on the implementation of the system itself, but also on the implementation and usage of the sub-solutions used, hardware and network constraints. Complex software architectures are usually designed for 24x7 service, often for millions of users, thus the current standards of cloud services, for example, availability and reliability are crucial.

Before we go any further let us define our terms \textit{complex software architecture}:

\begin{definition}\label{def:complexSoftwareArch}A Complex software architecture can be a service or an application which is built on and is using multiple software instances as sub-solutions, which means there are communications and dependencies between these software instances, necessary to fulfill the desired functionality for the architecture and which spreads across multiple machines.
\end{definition}

Most sub-solutions often have their own self-healing processes, if they go to a simple error states, it is possible to recover without a big impact on their overall quality, but in high demand environment\ms{,} you cannot only rely on that. Companies have many professionals (also called operators) who are responsible for quality of their services and who must deal with any problem which occurs. From a personal experience, it may take quite some time to dig into all log files and find out what caused a problem. Even though a solution to a problem may be simple and solved rather quickly, every downtime is expensive as, for instance, British Airlines experienced from the end of May of 2017\footnote{\url{https://www.cbronline.com/enterprise-it/ba-outage-understanding-true-cost-downtime/}}. The best practice is to avoid totally such situations or at least be warned in advance, so you have a chance to prevent it.

We would like to highlight a real work experience with a message broker and a consumer storing data to a NoSQL database. We could restart each of the broken nodes and connection would renew automatically, but when there is a network drop, connection between these two component gets corrupted and data stop flowing. In that case, the consumer still can ping the broker and has a valid session thus it does not try to renew the connection, even though the broker already dropped it. If that happens, new messages are still being produced but not processed, the manual restart of all consumers was needed. 

As it has already been mentioned, the best way to understand a software related problem is analysis of related log files. Those automatically generated text files contain messages designed by developers recording software states, action and status descriptions. And because the people who created the source code knows the most about potential problems, logs are the most valuable source of information. Unfortunately, it may also be tricky, since those messages are created by humans, other people do not have to understand them as they were intended to.

Since every solid program and consequentially every software architecture should be deterministic, there should always be signs which can lead to state classification or even be assigning a probability to possible following events and potentially problems. It means, software logs are perfect candidates for machine learning methods. Since the hardest analytic work would be designated to computers, operators do not have to spend their time with visiting each server and performing hand analysis of each log file to identify a root cause of an anomaly state they are facing.
For the purpose of this work let us define \textit{a potential anomaly state} of a software architecture:

\begin{definition}\label{def:potentilAnomalyState} The potential anomaly state of a software architecture can be each state which on a pre-set probabilistic level leads to a situation which endangers functionally of the software architecture.
\end{definition}

Keeping in mind the e-shop example we used before, there are not only negation anomalies a system can experience. In software systems, it is possible to see a positive anomaly such as if resources of a virtual machine are increased and therefore percentage of CPU usage goes down. This kind of information is, in general, less useful than negative anomalies, which can lead to potential problems. Another problem can be the false-positive ratio, since warnings about potential anomaly states cannot be too frequent otherwise it may overwhelm everybody who follows them.

Ultimately, in this thesis we would like to explore and summarize current approaches and designs for detection of anomalies in computer architecture behaviour, as well as suggest and test our own framework for a complex software architecture. There are many solutions around and even many good competitive commercial products, but still the results can be improved and there is a lot of research about this topic, so we decided to join the stream.

In chapter~\ref{chap:stateOfTheArt}, we summarize state of the art of console logs based anomaly detection as well as state of the art current log techniques and log message analysis and processing. We would like to give a reader at least a brief description of the algorithms and methods we are relying on with our implementation as well as to show what kind of performance could be expected from them. We are also in touch with leading log analysis and anomaly prediction companies and have tried to use their solution on existing datasets.

In the next section, chapter~\ref{chap:dataset}, we come up with our own dataset because we were not able to find any existing ones which would be close to our expectation of a complex software architecture. This new dataset was created in cooperation with a logging team of SAP Concur. Their solution consists of multiple each-to-other-dependant applications on hundreds of servers. You can also find here some statistics about their records and dependencies. This dataset is published with this thesis and some current state of the art frameworks were test on it. A brief summary of existing datasets can be also found in this section.

To fulfill the main goal, we also suggest how an ideal solution solutions could work, starting with how related data may be collected and stored. But the main change to the current solutions is in designing and developing of new machine learning approach for unsupervised and semi-supervised learning using graph-based structures and relation neural networks. All of our contribution is described in chapter \ref{chap:solution}.

Once our framework is fully functional, we are able to measure its performance on existing datasets as well as on our dataset from chapter~\ref{chap:dataset} and compare our results with some state of the art techniques from \ref{chap:stateOfTheArt}. These experiments, comparisons and their evaluations are located in chapter \ref{chap:experiments}.

This work was influenced and is a result of cooperation of multiple research institutions and successful companies. The main consultations were made with researches at the Czech Technical University in Prague and the National School of Computer Science and Applied Mathematics of Grenoble. We also consult results and techniques with experts from SAP Concur\footnote{\url{https://www.concur.com/}} and Blindspot\footnote{\url{http://blindspot-solutions.com/}}. All participants brought valuable additional value in form of experience, support and innovative ideas and there is a big thanks to all of them.  


