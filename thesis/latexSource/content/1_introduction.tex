\chapter{Introduction}

Complexity of software architectures grows in rapid speed and we cannot expect that possibility of a failure or potential problems is decreasing in the same mater. Therefore, modern systems are relying heavily on anomaly detection principles which can help with problem detection and in the best case even with their prediction. But letâ€™s first look at anomalies in the general way.

Anomalies, also referred in other publications as outliers, exceptions or aberrations, may be happening all the time everywhere around us and it depends only on our point of view whether we are able to distinguish them from standard behaviour of its context. An event itself is never an anomaly, to classify it we always need to know a context in which it appears. To be more general, Cambridge dictionary defines an anomaly as follows\footnote{\url{http://dictionary.cambridge.org/dictionary/english/anomaly}}:
\begin{center}\textit{A person or thing that is different from what is usual, or not in agreement with something else and therefore not satisfactory. }
\end{center}
But more fitting for us is a definition of an anomaly introduced in anomaly detection survey from 2009 \cite{chandola2009anomaly}:
\begin{center}
\textit{Anomalies are patterns in data that do not conform to a well-defined notion of normal behavior.}
\end{center}

It means, an anomaly may be a number of a time series which does not belong there according to expected rules or the probability of seeing is too low. It can be a state of a state space which has been classified as an anomaly in past or never seen before. It can be a sequence of actions which are not supposed to follow each other. And there are many other examples when a pattern does not fit to its context.

A nice general example of an anomaly would be an e-shop where every transaction is logged and a time line of transaction per day is drawn. Such line will have, most probably, a week pattern with an increasing trend and slight fluctuation. You would be able to predict an amount of transaction on a certain probability level for most of days in year. But for instance, on Black sale Friday the amount of transactions goes up. If an uninvolved person sees such time line without context, he can expect that the something went wrong, but an experienced business manager would know, it is normal and people are simply going more goods.

Monitoring and problematic of anomaly detection in different forms is studied cross multiple fields and domains. In statistic, the problematic has been studied for long time with roots in 19th century which can be seen in a historic paper \cite{edgeworth1887xli} from 1887. And there are many studies and implementations which deal with a form of anomaly or outlier detection so far. Let is highlight some to demonstrate possible implementation: \cite{salem2014online} introduces a health-care-focused alerting system for wear-able devices, \cite{rehak2008trust} detects network traffic anomalies to discovers security threads, credit card fraud detection is studied in \cite{bolton2001unsupervised} and there are many other studies. The one thing all implementations have in common is that every field demands a specific approach and specific background. In other words, there is no universal algorithm or a solution which would be applicable to all problems. In general, a deep knowledge of data and its understanding is the main key to find the best anomaly detection approach.

This thesis focuses on complex software architectures. Most current IT solutions are, as wall is built from bricks, built from multiple existing sub-component or applications- nobody develops from scratch, it is easier to use well-working sub-solution to achieve a new additional value. It means that, mandatory attributes like stability and quality of a new solutions do not only depend on implementation of the system itself, but also on implementation and usage of used sub-solutions, hardware and network status. Complex software architectures are usually designed for 24x7 service with possible millions of users so current standards of cloud services like availability and reliability are crucial.

Before we go any further let us define our term \textit{complex software architecture}:

\begin{definition}\label{def:complexSoftwareArch}Complex software architecture is a service or an application which is built on and using multiple software instances as its sub-solutions and there is a communication and dependency between these software instances to fulfill the right functionality of such architecture, and which spreads across multiple machines. 
\end{definition}

Most sub-solutions often have their own self-healing processes, if it goes to a simple error state, it is possible to recover without a big impact on its overall quality. But still, companies have many professionals (also called operators) who are responsible for quality of their services and who must deal with any problem which occurs. From a personal experience, it may take quit a time to dig into all log files to find out what caused a problem. Even though a solution to a problem may be simple and solved rather quickly, every downtime is expensive as, for instance, British Airlines experienced in the end of May of 2017\footnote{\url{https://www.cbronline.com/enterprise-it/ba-outage-understanding-true-cost-downtime/}}, so the best practice is to avoid totally such situations or at least be warned in advance, so you have a chance to prevent it.

We had a real work experience with a message broker and a consumer storing data to a NoSQL database. We could restart each of the broken nodes and connection would renew automatically, but when there is a network drop, connection between these two component gets corrupted and data stop flowing. In that case, the consumer still can ping the broker and has a valid session so does not try to renew the connection, but the broker already dropped it. If that happened, manual restart of all consumers was needed. 

As it has already been mentioned, the best way to understand a software related problem is analysis of related log files. Those automatically generated files contain messages designed by developers recording software states, action and status descriptions. And because the people who created the source code knows the best the potential problems, logs are the most valuable source of information. Unfortunately, it may also be tricky, since those messages are created by humans, other people do not have to understand them the original way.

Since every solid program consequentially every software architecture should be deterministic, there should always be signs which can lead to state classification of even to assigning a probability to possible following events potentially problems. It means, software logs are perfect candidates for machine learning methods. Since the hardest analytic work would be designated to computes, operators do not have to spend their time with visiting each server and hand analysis of each log file to identify a root cause of an anomaly state they are facing.
For the purpose of this work let's define \textit{potential anomaly state} of a software architecture:

\begin{definition}\label{def:potentilAnomalyState} potential anomaly state of a software architecture is every state which on pre-set probabilistic level leads to a situation which endangers functionally of the software architecture.
\end{definition}

Keeping in mind the e-shop example we used before, there not only negation anomalies a system can experience. For software system it is possible to see a positive anomaly such as if resources of a virtual machine are increased and therefore percentage of CPU usage goes down. But this kind of information is, in general, less useful then the negative anomalies which can lead to potential problems. On the other hand, warnings about potential anomaly states cannot be too frequent to not overwhelm anybody who follows them.

Ultimately, in this thesis we would like to explore and summarize current approaches and design of detection such anomalies for computer architectures, as well as suggest and test our own framework for a complex software architecture. There are many solutions around and even many good competitive commercial ones, but still the results can be improved and there is a lot of research about this topic, so we decided to join the stream.

In chapter \ref{chap:stateOfTheArt} we are summarizing the state of the art of anomaly detection as well as of current log techniques and log message analysis and processing. We would like to give a reader at least a brief description of algorithms and methods we are relying on with our implementation as well as show what kind of performance could be expected from them. This section also contains list of currently existing datasets. We are also in touch with leading log analysis and anomaly prediction companies and tried to use their solution on existing datasets.

In the next section, chapter \ref{chap:dataset}, we come up with our own dataset because we were not able to find any existing which would be close our expectation of complex software architecture at which we would like to focus. This new dataset was created in cooperation with a logging team of SAP Concur. Their solution consists of multiple each-to-other-depended application on hundreds of servers. You can also find here some statistics about the records and dependencies. This dataset is published with this thesis and some current state of the art framework were test on it.

To fulfill the main goal, we also suggest how the best solutions could work, starting with how related data may be collected and stored. But the main change to current solutions is designing and developing of new machine learning approached for unsupervised and semi-supervised learning using graph-based structures and relation neural networks. We attempted to defined what are the best input data and setting and combined it with other current techniques in chapter \ref{chap:solution}. After we had our framework we could test it on your dataset as well as on other public datasets in the same chapter.

This work was influenced and result of cooperation of multiple research institutions and successful companies. The main consultations were made with researches at the Czech Technical University in Prague and the National School of Computer Science and Applied Mathematics of Grenoble and from the companies we coopered with we would like to mention SAP Concur and Blindspot\footnote{\url{http://blindspot-solutions.com/}}. All participants brought valuable additional value in form of experience, support and innovative ideas and there is a big thanks to all of them.  
